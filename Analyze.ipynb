{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-01 19:36:02,990 - INFO - ------------------------------------------------------------------------\n",
      "2021-03-01 19:36:02,991 - INFO - ------------------Starting the log parsing process----------------\n",
      "2021-03-01 19:36:02,992 - INFO - ----------Seting up the environment...\n",
      "2021-03-01 19:36:02,997 - INFO - ----------Retrieving data from HDFS...\n",
      "2021-03-01 19:36:06,617 - INFO - ----------Processing data...\n",
      "2021-03-01 19:36:06,658 - INFO - ----------Saving results in Hive...\n",
      "2021-03-01 19:36:09,091 - INFO - ---------------------------Process Finished!!!--------------------------\n",
      "2021-03-01 19:36:09,092 - INFO - ------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import argparse\n",
    "import logging\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from dataclasses import dataclass\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#custom event frame definition           \n",
    "@dataclass\n",
    "class historic_event:\n",
    "            operation: str\n",
    "            details: str\n",
    "            date_time: datetime\n",
    "            duration: float\n",
    "                \n",
    "@dataclass\n",
    "class cumulated_event:\n",
    "            resource: str\n",
    "            description: str\n",
    "            way: str\n",
    "            timestamp: datetime\n",
    "                        \n",
    "#initial env setup \n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logging.info(\"------------------------------------------------------------------------\")\n",
    "logging.info(\"------------------Starting the log parsing process----------------\")\n",
    "logging.info(\"----------Seting up the environment...\")\n",
    "#better display for the dataframe in here!\n",
    "pd.set_option('display.width', 2000)\n",
    "parser = argparse.ArgumentParser(description='Job Arguments')\n",
    "parser.add_argument('--batchID')\n",
    "args, unknown = parser.parse_known_args()\n",
    "if args.batchID is None: \n",
    "    workingBatch = date.today().strftime(\"%H%m%d%Y\")\n",
    "else:\n",
    "    workingBatch = args.batchID\n",
    "workingPathHDFS = f'/user/akorobeinykov/korobeinykov-{workingBatch}' \n",
    "spark = SparkSession.builder.appName(\"OperationDurationAnalyze\").master('yarn').getOrCreate()\n",
    "sc = SparkContext.getOrCreate()\n",
    "#https://kb.databricks.com/jobs/spark-overwrite-cancel.html\n",
    "spark.conf.set(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\",\"true\")\n",
    "\n",
    "logging.info(\"----------Retrieving data from HDFS...\")\n",
    "df_main = spark.read.load(f'{workingPathHDFS}/parsed-*',\n",
    "           format=\"csv\", sep=\",\", header=\"true\", encoding=\"utf-8\").toPandas()\n",
    "\n",
    "logging.info(\"----------Processing data...\")\n",
    "df_main['timestamp'] = pd.to_datetime(df_main['timestamp'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "df_main = df_main.sort_values(by=['timestamp']).reset_index(drop=True) #agreed that operations are sequentioal and should be naturally sorted if not already\n",
    "final_events = []\n",
    "for index, row in df_main.iterrows():\n",
    "    if row['start']!='false' and 'operationStarted' not in row['operationDetail']:\n",
    "        if 'shuttle_move' in row['operation']:\n",
    "            way =' (' + row['stationOrigin'] + ' > ' + row['stationDestination'] + ')'\n",
    "        else:\n",
    "            way=''\n",
    "        if 'tagReceived' in row['operation']:\n",
    "            description = 'End'\n",
    "        else:\n",
    "            description = row['operation']\n",
    "        \n",
    "        final_events.append(cumulated_event(row['resource'], description, way, row['timestamp']))\n",
    "            \n",
    "df_all = pd.DataFrame(final_events)\n",
    "    \n",
    "#agreed, the events for one resource can not overlap - it should be safe to get duration as \"input-end-time\" - \"output-actionDefinition-time\"\n",
    "final_events_deltas = []\n",
    "for index, row in df_all.iterrows():\n",
    "    if 'End' not in row['description']:\n",
    "        for i, item in df_all[index+1:len(df_all)].iterrows():\n",
    "        #iterate on same list from current position until finds an end for this event\n",
    "            if (item['resource']==row['resource']) & ('End' in item['description']): \n",
    "                final_events_deltas.append(historic_event(str(row['resource']+' - '+row['description']), row['way'],row['timestamp'],(item['timestamp']-row['timestamp'])))\n",
    "                break\n",
    "                   \n",
    "df_deltas = pd.DataFrame(final_events_deltas)\n",
    "df_deltas['duration']=df_deltas['duration'].astype('timedelta64[ms]')/1000\n",
    "df_deltas = df_deltas.sort_values(by=['operation', 'details', 'date_time']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "#get the averages before this-----------------------------------------------------------------------------------------------------------\n",
    "logging.info(\"----------Saving results in Hive...\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS eventDurationHistoryData (operation string, details string, date_time timestamp, duration float) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\")\n",
    "spark.createDataFrame(df_deltas).write.mode(\"append\").format(\"hive\").saveAsTable(\"default.eventDurationHistoryData\")\n",
    "\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS eventDurationHistoryData{workingBatch} (operation string, details string, date_time timestamp, duration float) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\")\n",
    "spark.createDataFrame(df_deltas).write.mode(\"overwrite\").format(\"hive\").saveAsTable(f\"default.eventDurationHistoryData{workingBatch}\")\n",
    "\n",
    "logging.info(\"---------------------------Process Finished!!!--------------------------\")\n",
    "logging.info(\"------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "python",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
