{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-01 19:41:11,025 - INFO - ------------------------------------------------------------------------\n",
      "2021-03-01 19:41:11,026 - INFO - ---------------------Starting the log parsing process-------------------\n",
      "2021-03-01 19:41:11,027 - INFO - ----------Seting up the environment...\n",
      "2021-03-01 19:41:11,032 - INFO - ----------Processing log file ['example', '.xes'] ...\n",
      "2021-03-01 19:41:11,033 - INFO - ----------Parsing the data from XES log file into the dataframe...\n",
      "2021-03-01 19:41:11,040 - INFO - ----------Saving the data to CSV on HDFS...\n",
      "2021-03-01 19:41:11,492 - INFO - ----------Processing log file ['EventLogTestbedMain_2020-10-23_10-15-52', '.xes'] ...\n",
      "2021-03-01 19:41:11,495 - INFO - ----------Parsing the data from XES log file into the dataframe...\n",
      "2021-03-01 19:41:11,502 - INFO - ----------Saving the data to CSV on HDFS...\n",
      "2021-03-01 19:41:11,916 - INFO - ----------Processing log file ['example-empty', '.xes'] ...\n",
      "2021-03-01 19:41:11,918 - INFO - ----------Parsing the data from XES log file into the dataframe...\n",
      "2021-03-01 19:41:11,922 - INFO - ----------No logs found in file: ['example-empty', '.xes'] ! Excluding file...\n",
      "2021-03-01 19:41:11,923 - INFO - ----------Cleaning the original Log file: ['example', '.xes']\n",
      "2021-03-01 19:41:11,924 - INFO - ----------Cleaning the original Log file: ['EventLogTestbedMain_2020-10-23_10-15-52', '.xes']\n",
      "2021-03-01 19:41:11,924 - INFO - ----------Cleaning the original Log file: ['example-empty', '.xes']\n",
      "2021-03-01 19:41:11,925 - INFO - ---------------------------Process Finished!!!--------------------------\n",
      "2021-03-01 19:41:11,926 - INFO - ------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from dataclasses import dataclass\n",
    "import xml.etree.ElementTree as file\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "#custom event frame definition\n",
    "@dataclass\n",
    "class extracted_event:\n",
    "            eventtype: str\n",
    "            resource: str\n",
    "            operation: str\n",
    "            operationDetail: str\n",
    "            stationOrigin: str\n",
    "            stationDestination: str\n",
    "            start: str\n",
    "            timestamp: datetime\n",
    "\n",
    "                \n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logging.info(\"------------------------------------------------------------------------\")\n",
    "logging.info(\"---------------------Starting the log parsing process-------------------\")\n",
    "logging.info(\"----------Seting up the environment...\")\n",
    "parser = argparse.ArgumentParser(description='Job Arguments')\n",
    "parser.add_argument('--batchID')\n",
    "args, unknown = parser.parse_known_args()\n",
    "if args.batchID is None: \n",
    "    workingBatch = date.today().strftime(\"%H%m%d%Y\")\n",
    "else:\n",
    "    workingBatch = args.batchID\n",
    "workingPathHDFS = f'/user/akorobeinykov/korobeinykov-{workingBatch}' \n",
    "sourceDir = '/home/akorobeinykov/-MSc-QSE-Korobeinykov-/Data'\n",
    "originalFileNames = []\n",
    "\n",
    "\n",
    "\n",
    "#read all files from the directory\n",
    "for fileName in os.listdir(sourceDir):\n",
    "    if fileName.endswith(\".xes\"): \n",
    "        #originalFileNames.append(os.path.abspath(filename))\n",
    "        originalFileNames.append([os.path.splitext(fileName)[0],os.path.splitext(fileName)[1]])\n",
    "        continue\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "spark = SparkSession.builder.appName(\"AK-CleanAndInsert\").master('local').getOrCreate()\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "#iterating over all files in the log directory\n",
    "for fileName in originalFileNames:\n",
    "    logging.info(f\"----------Processing log file {fileName} ...\")\n",
    "    logging.info(\"----------Parsing the data from XES log file into the dataframe...\")\n",
    "    # get the events and the data into a pandas frame\n",
    "    tree = file.parse(f'{sourceDir}/{fileName[0]}{fileName[1]}')\n",
    "    root = tree.getroot()\n",
    "    extracted_events = []\n",
    "    try:\n",
    "        for event in root.findall('.//trace/event'):\n",
    "            if event.find('./string[@key=\"operationType\"]')!=None:\n",
    "                    operationType=event.find('./string[@key=\"operationType\"]').get('value')\n",
    "                    #searching with regex for the first part before the dot - the resourceID\n",
    "                    resourceName=re.search(r\"^[^.]*\", event.find('./string[@key=\"org:resource\"]').get('value')).group(0)\n",
    "                    operation=event.find('./string[@key=\"operationName\"]').get('value')\n",
    "                    detail=event.find('./string[@key=\"concept:name\"]').get('value')\n",
    "                    timestamp=event.find('./date[@key=\"time:timestamp\"]').get('value')\n",
    "                    so='-'\n",
    "                    sd='-'                    \n",
    "                    if ('shuttle' in resourceName) & (operationType=='MESoutput'):\n",
    "                        try:\n",
    "                            #adds the origin and destination for shuttle movenent if they exist, if not - ignore both set to \"-\"\n",
    "                            so=event.find('./string[@key=\"StationIdOrigin\"]').get('value')\n",
    "                            sd=event.find('./string[@key=\"StationIdDestination\"]').get('value')\n",
    "                        except:\n",
    "                            for string in event.findall('.//string'):\n",
    "                                if 'StationIdDestination' in string.get('key'):\n",
    "                                    sd=string.get('value')\n",
    "                     \n",
    "                    #will be parsing only responses from MES, as output is not relevant for this - reduced effort    \n",
    "                    if (operationType=='MESinput') :\n",
    "                        for string in event.findall('.//string'):\n",
    "                            if  ('OperationStarted' in string.get('key')) | ('OperationFinished' in string.get('key')):\n",
    "                                start=string.get('value')\n",
    "                    else:\n",
    "                        start='-'\n",
    "                    #filling in final row object\n",
    "                    extracted_events.append(extracted_event(operationType,resourceName,operation,detail,so,sd,start,timestamp))\n",
    "        #converting the list into the dataframe\n",
    "        df = pd.DataFrame(extracted_events)\n",
    "        if df.empty!=True:     \n",
    "            #only when log file was parsable and resulted in at least one valid event we add it to parsed events in hadoop\n",
    "            logging.info(\"----------Saving the data to CSV on HDFS...\")\n",
    "            spark.createDataFrame(df).coalesce(1).write.mode('overwrite').option('header','true').csv(f'{workingPathHDFS}/parsed-{fileName[0]}')\n",
    "            #spark.createDataFrame(tree).coalesce(1).write.mode(\"overwrite\").save(f'{workingPathHDFS}/parsed-{fileName[0]}')\n",
    "        else:\n",
    "            logging.info(f\"----------No logs found in file: {fileName} ! Excluding file...\")\n",
    "    except Exception as error:\n",
    "        logging.info(f\"----------No logs found in file: {fileName}! Excluding while: {error}\")\n",
    "\n",
    "\n",
    "#iterating over all original log files and do something?\n",
    "for fileName in originalFileNames:\n",
    "    logging.info(f\"----------Cleaning the original Log file: {fileName}\")\n",
    "    w=fileName[0] + fileName[1]\n",
    "    \n",
    "#!echo \"----------Listing the created directories...\"\n",
    "#!hadoop fs -ls -R $workingPathHDFS/ \n",
    "\n",
    "logging.info(\"---------------------------Process Finished!!!--------------------------\")\n",
    "logging.info(\"------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Starting the log parsing process-----------\n",
      "----------Seting up the environment\n",
      "/home/akorobeinykov/-MSc-QSE-Korobeinykov-/example.xes\n",
      "/home/akorobeinykov/-MSc-QSE-Korobeinykov-/example-1.xes\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "python",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
